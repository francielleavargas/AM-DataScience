# -*- coding: utf-8 -*-
"""Exercício - Seleção de Variáveis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QhZZVXR6OFV5VTgl4Ldv46RX1JHlo1wT

# Exercício 03
## Seleção de variáveis
### Alunos:

- Francielle Alves Vargas, Nusp: 9527629


---

Nesse exercício utilizaremos um dataset para classificação binária. No total são 19 variáveis de entrada e 1 variável de saída. O dataset pode ser encontrado no arquivo "dataset_hepatitis.csv".

Existem valores faltantes nesse dataset, representados pelo caractere "?".

**Há material complementar disponível para resolução desse exercício. Acesse o arquivo "Transformações em dados heterogeneos" antes de prosseguir.**

### Questão 01.

Carregue o dataset (`pandas.read_csv`) e interprete os valores ausentes corretamente. Utilize o parâmetro `na_values`.
"""

import pandas as pd
import numpy as np
dados_originais = pd.read_csv("dataset_hepatitis.csv", na_values=['?'])

#Apresenta informações do dataset e valores ausentes
#df.info()
#df.isnull()
print(dados_originais)

#Separando as features
X = df.iloc[:, :-1]
print(X)

#Separando a classe
Y = df.iloc[:,19]
print(Y)

"""---

### Questão 02.

Nessa questão, você irá fazer o pré-processamento dos dados.

Você deve:


*   **Para os dados numéricos:** substitua os valores faltantes utilizando a estratégia de média (`sklearn.impute.SimpleImputer`). Depois padronize o intervalo dessas variáveis (`sklearn.preprocessing.StandardScaler`)
*   **Para os dados categóricos:** substitua os valores faltantes utilizando a estratégia de mais frequentes (`sklearn.impute.SimpleImputer`). Depois converta os dados categóricos para uma representação numérica (`sklearn.preprocessing.OneHotEncoder`),

Lembre-se de utilizar as classes `sklearn.pipeline.Pipeline` e `sklearn.compose.ColumnTransformer` conforme consta no material complementar.
"""

import pandas as pd 
import numpy as np 
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer

# Criamos um vetor com o nome das classes desejadas
features_numericos = ['AGE', 'BILIRUBIN', 'ALK_PHOSPHATE', 'SGOT', 'ALBUMIN', 'PROTIME']
features_categoricos = ['SEX', 'STEROID', 'ANTIVIRALS', 'FATIGUE', 'MALAISE', 'ANOREXIA', 'LIVER_BIG', 'LIVER_FIRM', 'SPLEEN_PALPABLE', 'SPIDERS', 'ASCITES', 'VARICES', 'HISTOLOGY']

pipeline_numerico = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())])

pipeline_categorico = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder())])

transformacao = ColumnTransformer(
    transformers=[
        ('transformacao numerica', pipeline_numerico, features_numericos),
        ('transformacao categorica', pipeline_categorico, features_categoricos),        
    ])

dados_transformados = transformacao.fit_transform(X)
dados_transformados.round(2)

"""---

### Questão 03.

Determine as 10 variáveis mais relevantes do conjunto (lembre-se de concatenar a variável alvo no dataset transformado). Você pode utilizar a matriz de covariância (`numpy.cov`), correlação (`pandas.DataFrame.corr`), `scatterplot`, etc. Justifique sua escolha.

Lembre-se que heatmaps (`seaborn.heatmap`) podem auxiliar a visualização.
"""

import seaborn as sns
import matplotlib.pyplot as plt

dados_transformados_com_classe = np.c_[dados_transformados, df.iloc[:, -1]]

data_t = pd.DataFrame(dados_transformados_com_classe)
correlation_matrix = data_t.corr(method="pearson").round(2)
sns.heatmap(data=correlation_matrix, annot=True)
plt.rcParams['figure.figsize'] = [25, 20]

#Selecionei as dez variáveis mais relevantes considerando como critério a seguinte escala:
#<0 Nenhuma relevância
#0-0,19 relevância pobre
#0,20-0,39 relevância leve
#0,40-0,59 relevância moderada
#0,60-0,79 relevância substancial
#0,80-1,00 relevância quase perfeita

#Extrai as 10 variáveis mais relevantes
correlacao_com_variavel_alvo = correlation_matrix.iloc[:,-1]
indice_10_variaveis = np.argpartition(correlacao_com_variavel_alvo, -10)[-10:]

"""---

### Questão 04.

Com as 10 varíaveis determinadas na questão anterior, crie um novo conjunto de dados.

Lembre-se que o numpy permite indexar colunas/linhas utilizando arrays.
"""

#cria um novo conjunto de dados com as dez variáveis mais relevantes
data_10_variaveis = pd.DataFrame(indice_10_variaveis)
print(data_10_variaveis)

"""---

### Questão 05.

Reduza a dimensionalidade do conjunto de dados criado utilizando a técnica PCA (`sklearn.decomposition.PCA`). Reduza a dimensionalidade de forma que no mínimo 80% da variância dos dados seja mantida (atente-se ao atributo  `explained_variance_ratio_`). Depois exiba os eixos principais.
"""

from sklearn.decomposition import PCA

pca = PCA(n_components=5)
pca.fit(correlation_matrix)
X_pca = pca.transform(correlation_matrix)
print("Original shape:   ", correlation_matrix.shape)
print("Transformed shape:", X_pca.shape)
print(pca.explained_variance_)
print(pca.explained_variance_ratio_.sum())

"""---

### Questão 06.

Teste um modelo de classificação (através da função `model_tester`) nos conjuntos criados nas questões 4 e 5. Houve redução expressiva da acurácia?
"""

from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import cross_val_score
from sklearn.decomposition import PCA

# - Descrição:  Função que testa um modelo de classificação (MLP). 
# - Parâmetros: Variáveis de entrada (inputs) e a variável alvo (target).
# - Retorna:    Não retorna nenhum valor. Mas imprime a acurácia do modelo.
def model_tester(inputs, target):
  clf = MLPClassifier(hidden_layer_sizes=(5,), random_state=0, max_iter=10000)
  scores = cross_val_score(clf, inputs, target, cv=10)
  print("Score do classificador: %.2f" % (scores.mean()*100))

target = correlation_matrix.iloc[:,-1]

model_tester(X_pca, target)